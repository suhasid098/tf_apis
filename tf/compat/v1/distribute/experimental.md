description: Experimental Distribution Strategy library.

<div itemscope itemtype="http://developers.google.com/ReferenceObject">
<meta itemprop="name" content="tf.compat.v1.distribute.experimental" />
<meta itemprop="path" content="Stable" />
</div>

# Module: tf.compat.v1.distribute.experimental

<!-- Insert buttons and diff -->

<table class="tfo-notebook-buttons tfo-api nocontent" align="left">

</table>



Experimental Distribution Strategy library.



## Classes

[`class CentralStorageStrategy`](../../../../tf/compat/v1/distribute/experimental/CentralStorageStrategy.md): A one-machine strategy that puts all variables on a single device.

[`class CollectiveCommunication`](../../../../tf/distribute/experimental/CommunicationImplementation.md): Cross device communication implementation.

[`class CollectiveHints`](../../../../tf/distribute/experimental/CollectiveHints.md): Hints for collective operations like AllReduce.

[`class CommunicationImplementation`](../../../../tf/distribute/experimental/CommunicationImplementation.md): Cross device communication implementation.

[`class CommunicationOptions`](../../../../tf/distribute/experimental/CommunicationOptions.md): Options for cross device communications like All-reduce.

[`class MultiWorkerMirroredStrategy`](../../../../tf/compat/v1/distribute/experimental/MultiWorkerMirroredStrategy.md): A distribution strategy for synchronous training on multiple workers.

[`class ParameterServerStrategy`](../../../../tf/compat/v1/distribute/experimental/ParameterServerStrategy.md): An asynchronous multi-worker parameter server tf.distribute strategy.

[`class TPUStrategy`](../../../../tf/compat/v1/distribute/experimental/TPUStrategy.md): TPU distribution strategy implementation.

